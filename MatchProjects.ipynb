{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import urllib2\n",
    "from selenium import webdriver  \n",
    "from selenium.common.exceptions import NoSuchElementException  \n",
    "from selenium.webdriver.common.keys import Keys  \n",
    "import bs4 as bs\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this jupyter Notebook we will try to find lexical matches between doctoral projects in the Humanities and Social Sciences Group at the University of Leuven. As a first step, we will scrape the Research database at <http://www.kuleuven.be/research/researchdatabase/faculty/all.htm>, and compile a list of current PhD projects. Next, we will use machine learning tools to search for correspondence between different projects. The goal is to bring together PhD students from different research units working on related topics or using similar methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer:* Scraped from the URL's below on Aug 23, 2016. No warranty that the results will be accurate. They could be irrelevant, outdated, hilarious, or trivial. But for some PhD students it might give insights. If you don't get any results, maybe your name is not in the database, or you have not uploaded a title and/or abstract for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Research Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The faculties listed at <http://www.kuleuven.be/research/researchdatabase/faculty/all.htm> do not all belong to the Group of Humanities and Social Sciences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List faculties belonging to the group of humanities:\n",
    "#    50000102    Faculty of Theology and Religious Studies\n",
    "#    50000130    Institute of Philosophy\n",
    "#    50000146    Faculty of Canon Law\n",
    "#    50000148    Faculty of Law\n",
    "#    50000208    Faculty of Economics and Business (FEB)\n",
    "#    50000243    Faculty of Social Sciences\n",
    "#    50000275    Faculty of Arts\n",
    "#    50000339    Faculty of Psychology and Educational Sciences\n",
    "              \n",
    "Humanities = {50000102, 50000130, 50000146, 50000148, 50000208, 50000243, 50000275, 50000339}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "projects = []\n",
    "\n",
    "for faculty in Humanities:\n",
    "    url = 'http://www.kuleuven.be/research/researchdatabase/faculty/' + str(faculty) + '.htm'\n",
    "    print url\n",
    "    source = urllib2.urlopen(url).read()\n",
    "    start_project_url = 1\n",
    "    while start_project_url>0:\n",
    "        start_project_url = source.find('/research/researchdatabase/project/')\n",
    "        source = source[start_project_url:]\n",
    "        end_project_url = source.find('.htm')\n",
    "        if end_project_url>0:\n",
    "            project = 'http://www.kuleuven.be' + source[0:end_project_url+4]\n",
    "            print project           \n",
    "            projects.append(project)\n",
    "        source = source[end_project_url:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`projects` is now a list containing the URLs to all individual research project pages. We will now iterate that list and extract relevant information from each page. We use the selenium package to read JavaScript content rendered in the browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_non_ascii(string):\n",
    "    string = string.format('ascii')\n",
    "    string = string.replace('\\\\u2019',\"'\")\n",
    "    string = string.replace('\\\\u2018',\"'\")\n",
    "    string = string.replace('<br/>',\" \")\n",
    "    string = string.replace('<i>',\"\")\n",
    "    string = string.replace('</i>',\"\")\n",
    "    string = string.replace('\\\\xa0',\" \")\n",
    "    string = string.replace('<p align=\"LEFT\">',\"\")\n",
    "    string = string.replace('</p>',\"\")\n",
    "    string = string.replace('<p>',\"\")\n",
    "    string = string.replace('&amp;',\" and \")\n",
    "    string = string.replace('\\u201c',\"\")\n",
    "    string = string.replace('\\u201d',\"\")\n",
    "    string = string.replace('\\\\n',\" \")\n",
    "    string = string.replace('\\\\t',\" \")\n",
    "    string = string.replace('\\\\u2013',\" \")\n",
    "    string = string.replace('<em>',\" \")\n",
    "    string = string.replace('</em>',\" \")\n",
    "    string = string.replace('<span lang=\"EN-US\">',\" \")\n",
    "    string = string.replace('<span>',\" \")\n",
    "    string = string.replace('</span>',\" \")\n",
    "    string = string.replace('<span>',\" \")\n",
    "    string = string.replace('\\\\xe0',\"à\")\n",
    "    string = string.replace('\\\\u02bb',\"\")\n",
    "    string = string.replace('\\\\u02bc',\"\")\n",
    "    string = string.replace('\\\\u2014',\" \")\n",
    "    string = string.replace('<\\\\xf6',\" \")\n",
    "    string = string.replace('<\\\\xab',\" \")\n",
    "    string = string.replace('<\\\\xbb',\" \")\n",
    "    string = string.replace('<strong>',\" \")\n",
    "    string = string.replace('<\\strong>',\" \")\n",
    "    string = string.replace('\\\\xe8',\"è\")\n",
    "    string = string.replace('\\\\xc3\\xa8',\"e\")\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Firefox()\n",
    "Projects_List = []\n",
    "\n",
    "start_idx = 0\n",
    "\n",
    "for idx, project in enumerate(projects[start_idx:]):\n",
    "    print str(start_idx+idx+1) + \" / \" + str(len(projects))\n",
    "    # Project URL:\n",
    "    project_url = project\n",
    "       \n",
    "    browser.get(project)\n",
    "    time.sleep(3)\n",
    "    html_source = browser.page_source\n",
    "    soup = bs.BeautifulSoup(html_source)\n",
    "    \n",
    "    # Content:\n",
    "    content = soup.find_all(\"div\", {\"class\": \"grid spacer ng-scope\"})\n",
    "\n",
    "    # Check whether person is still listed in who-is-who:\n",
    "    person_missing = 0\n",
    "    for content_part in content:\n",
    "        contents = str(content_part.get_text)\n",
    "        if contents.find('Doctorandus')>0:\n",
    "            start = contents.find(\"href=\")+6\n",
    "            stop = contents.find(\"title\")-2\n",
    "            who_is_who = contents[start:stop]\n",
    "            if who_is_who == \"http://www.kuleuven.be/wieiswie/nl/person/\":\n",
    "                person_missing = 1\n",
    "    if person_missing:\n",
    "        print project_url + \" MISSING\"\n",
    "        continue\n",
    "    print project_url\n",
    "    \n",
    "    project_summ = \"\"\n",
    "    name = \"\"\n",
    "    who_is_who = \"\"\n",
    "    u_number = \"\"\n",
    "    for content_part in content:\n",
    "        contents = str(content_part.get_text)\n",
    "        if contents.find('item.summary')>0:\n",
    "            project = str(content_part.get_text)\n",
    "            start = project.find('ng-bind-html=\"item.summary\"')+28\n",
    "            project = project[start:]\n",
    "            stop = project.find('</div')\n",
    "            project_summ = project[:stop]\n",
    "            try:\n",
    "                project_summ = replace_non_ascii(project_summ)\n",
    "            except:\n",
    "                project_summ = project_summ\n",
    "        elif contents.find('Doctorandus')>0:\n",
    "            name = content_part.contents[1].getText()\n",
    "            start = contents.find(\"href=\")+6\n",
    "            stop = contents.find(\"title\")-2\n",
    "            who_is_who = contents[start:stop]\n",
    "            u_number = who_is_who[who_is_who.find(\"person/\")+7:]\n",
    "        \n",
    "    # Title:   \n",
    "    title = soup.find_all(\"div\",{\"class\":\"grid__12 grid--bp-med__9\"})    \n",
    "    title = str(title)\n",
    "    start = title.find('h3 class=\"ng-binding\">')+22\n",
    "    title = title[start:]\n",
    "    stop = title.find(\"(\")\n",
    "    title = title[:stop-1]\n",
    "    \n",
    "    if (len(title)+len(project_summ))>10:\n",
    "        Projects_List.append([project_url, name, u_number, who_is_who, title, project_summ])\n",
    "        # writing to file after each new entry, because of server time-outs\n",
    "        pickle.dump(Projects_List, open( \"Projects_List_2.p\", \"wb\"))\n",
    "        \n",
    "browser.quit()\n",
    "print len(Projects_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-format the projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge the separate Project-lists (separate due to server time-outs):\n",
    "Projects_0 = pickle.load(open(\"Projects_List_0.p\", \"rb\"))\n",
    "Projects_1 = pickle.load(open(\"Projects_List_1.p\",\"rb\"))\n",
    "Projects_2 = pickle.load(open(\"Projects_List_2.p\",\"rb\"))\n",
    "\n",
    "Projects_List = Projects_0 + Projects_1 + Projects_2\n",
    "pickle.dump(Projects_List, open( \"Projects_List.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Projects_List = []\n",
    "import pickle\n",
    "Projects_List = pickle.load( open( \"Projects_List.p\", \"rb\" ) )\n",
    "Descriptions = [project[4]+' '+project[5] for project in Projects_List] # merge title and project description\n",
    "Students = [project[1] for project in Projects_List] # list of students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print Projects_List[1722]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find lexical similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a tokenized version of the descriptions\n",
    "# (1) recode to ASCII, ignoring non-ASCII characters\n",
    "# (2) make all text lowercase\n",
    "# (3) remove punctuation (except for the \"-\" character)\n",
    "# (4) remove stopwords from English language\n",
    "# (5) remove general research-related tokens\n",
    "\n",
    "import nltk # Natural language toolkit\n",
    "import string\n",
    "#print string.punctuation\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords # run \"nltk.download('stopwords')\" once\n",
    "research_words = ['research', 'result', 'conclusion', 'conclude', 'show', 'investigate', 'study', 'project', 'phd',\n",
    "                  'understanding', 'goal', 'aim', 'aims', 'process', 'processing', 'results', 'theory', 'effect',\n",
    "                  'effects', 'test', 'testing', 'data', 'analysis', 'analyses', 'hypothesis', 'hypotheses',\n",
    "                  'subject', 'subjects', 'participant', 'participants', 'role', 'variable', 'variables', \n",
    "                  'finding', 'findings', 'found', 'shows', 'showed', 'shown', 'researchers', 'significant',\n",
    "                  'significance', 'discussion', 'theories', 'studies', 'chapter', 'influence', 'influences',\n",
    "                  'evidence', 'studied', 'doctoral', 'thesis', 'find', 'finds', 'underlying', 'approach']\n",
    "useless_words = ['however', 'whether', 'recent', 'other', 'use', 'imply', 'current', 'currently', 'aspect',\n",
    "                 'aspects', 'new', 'field', 'versus', 'also', 'possibility', 'towards', 'thus', 'hence', 'as', \n",
    "                 'general', 'using', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine',\n",
    "                 'ten', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'discuss', 'discussed', 'possible', \n",
    "                 'important', 'importance', 'understand', 'understaning', 'used', 'with', 'without', 'would', 'will', \n",
    "                 'many', 'most', 'previous', 'present', 'among', 'common', 'described', 'presented', 'reflect',\n",
    "                 'reflects', 'vs', 'get', 'gets', 'getting', 'back', 'main', 'although', 'may', 'account', 'therefore',\n",
    "                 'upon', 'eg', 'e.g.', 'usually', 'despite', 'certain', 'seem', 'seems', 'obvious', 'related',\n",
    "                 'must', 'within',\n",
    "                 'differ', 'different', 'could', 'clearly', 'depend', 'depends', 'way', 'propose', 'high',\n",
    "                 'low', 'specific', 'indeed', 'furthermore', 'afterwards', 'allow', 'us', 'around', 'others',\n",
    "                 'particular', 'de', 'en', 'het', 'together', 'along', 'goal', 'goals', 'nevertheless'] + research_words\n",
    "\n",
    "stopwords_rem = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "\n",
    "token_dict = {}\n",
    "for ind, description in enumerate(Descriptions):\n",
    "    #description = description.encode('ascii', 'ignore') \n",
    "    lowers = description.lower()\n",
    "    no_punctuation = lowers.translate(None,string.punctuation.replace(\"-\", \"\"))\n",
    "    no_stopwords = stopwords_rem.sub('', no_punctuation)\n",
    "    no_uselesswords = ' '.join([i for i in no_stopwords.split() if i not in useless_words])\n",
    "    token_dict[ind] = no_uselesswords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "# Run the TF-IDF algorithm on the tokened descriptions\n",
    "# TF-IDF = \"term frequency–inverse document frequency\"\n",
    "tfidf = TfidfVectorizer(tokenizer=LemmaTokenizer())\n",
    "tfs = tfidf.fit_transform(token_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a CSR, compressed sparse row format, matrix. The rows are the original descriptions, the columns are the features (i.e., tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "#print feature_names[26073]\n",
    "print len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "student_id = 1129\n",
    "student_vector = tfs[student_id]\n",
    "max_loads = np.argsort(student_vector.data)[::-1]\n",
    "print 'Most important \"tokens\" for PhD student '+Students[student_id]+':'\n",
    "for f in np.arange(min(len(max_loads),10)):\n",
    "    print feature_names[student_vector.indices[max_loads[f]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the 2000+ documents, we can compare the tfs vectors. To find the **cosine distances** of one document *D* to all of the others we need to compute the dot products of the *D*-th vector with all of the others (tfs vectors are already row-normalized). To get the first vector we need to slice the matrix row-wise to get a submatrix with a single row:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn provides pairwise metrics that work for both dense and sparse representations of vector collections. In this case we need a dot product that is also known as the linear kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_matches = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities_all = linear_kernel(tfs,tfs)\n",
    "cosine_similarities_all = cosine_similarities_all.argsort()\n",
    "# Note: the argsort returns one student more than once as the best corresponding one (missing values for specific students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print Top_K[1129]\n",
    "print Projects_List[1060][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Top_K =  [student[:-(n_matches+2):-1] for student in cosine_similarities_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for student in cosine_similarities_all:\n",
    "    if student[-1]==student_id:\n",
    "        print student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 0 = project url\n",
    "# 1 = student name\n",
    "# 2 = student number\n",
    "# 3 = student url\n",
    "# 4 = title\n",
    "# 5 = abstract\n",
    "\n",
    "all_students = []\n",
    "for student in Top_K:\n",
    "    new_row = []\n",
    "    student_u    = (Projects_List[student[0]][2]).lower()\n",
    "    student_name = (Projects_List[student[0]][1]).title()\n",
    "    student_url  = (Projects_List[student[0]][3])\n",
    "    new_row.append(student_u)\n",
    "    new_row.append(student_name)\n",
    "    new_row.append(student_url)\n",
    "    for match in np.arange(n_matches):\n",
    "        match_name  = (Projects_List[student[match+1]][1]).title()\n",
    "        match_url   = (Projects_List[student[match+1]][0])\n",
    "        match_title = (Projects_List[student[match+1]][4])\n",
    "        new_row.append(match_name)\n",
    "        new_row.append(match_url)\n",
    "        new_row.append(match_title)\n",
    "    all_students.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print all_students[1021]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence to find the top 3 related documents, we can use argsort and some negative array slicing (most related documents have highest cosine similarity values, hence at the end of the sorted indices array):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_students[1021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"output.csv\", \"wb\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(all_students)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
